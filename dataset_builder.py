# dataset_builder.py - Construction de datasets pour entraÃ®nement ML

"""
Dataset Builder pour APEX ML System

ResponsabilitÃ©s:
- RÃ©cupÃ©rer un historique massif de bougies (50k+)
- Calculer tous les indicateurs sur l'historique
- Extraire les features standardisÃ©es
- Labelliser chaque sample (WIN/LOSS basÃ© sur TP/SL virtuel)
- Sauvegarder le dataset au format CSV/pickle

Usage:
    python dataset_builder.py --symbol ETH/USDT --timeframe 1m --limit 50000
    python dataset_builder.py --multi BTC/USDT,ETH/USDT,BNB/USDT --limit 20000
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import ml_config as ml_config
from feature_extractor import FeatureExtractor
from logger_apex import get_logger
from data_collector_apex import DataCollectorApex
from indicators_advanced import AdvancedIndicators
from support_resistance_detector import SupportResistanceDetector
import os
from datetime import datetime
import pickle
import time

class DatasetBuilder:
    """
    Constructeur de datasets pour ML

    V2.0: Complet avec fetch + labellisation TP/SL + multi-crypto
    """

    def __init__(self):
        """Initialise le builder"""
        self.logger = get_logger()
        self.feature_extractor = FeatureExtractor()
        self.data_collector = DataCollectorApex()
        self.indicators = AdvancedIndicators()
        self.sr_detector = SupportResistanceDetector()

        # CrÃ©er le dossier datasets
        if not os.path.exists(ml_config.DATASET_OUTPUT_DIR):
            os.makedirs(ml_config.DATASET_OUTPUT_DIR)

        self.logger.info("âœ… DatasetBuilder initialisÃ©")

    def build_dataset(self, symbol: str, timeframe: str, limit: int = 50000) -> Optional[str]:
        """
        Construit un dataset complet pour UN symbole

        Args:
            symbol: Symbole (ex: ETH/USDT)
            timeframe: Timeframe (ex: 1m)
            limit: Nombre de bougies Ã  rÃ©cupÃ©rer

        Returns:
            Chemin vers le fichier dataset crÃ©Ã©, ou None si erreur
        """
        self.logger.info(f"ğŸ”¨ Construction dataset: {symbol} {timeframe} ({limit} bougies)")
        print(f"\n{'='*70}")
        print(f"ğŸ”¨ CONSTRUCTION DATASET".center(70))
        print(f"{'='*70}")
        print(f"\nğŸ“Š Symbole: {symbol}")
        print(f"â±ï¸  Timeframe: {timeframe}")
        print(f"ğŸ“ˆ Bougies: {limit}")

        # 1. FETCH DATA
        print(f"\n[1/5] ğŸ“¥ TÃ©lÃ©chargement donnÃ©es historiques...")
        df = self._fetch_historical_data(symbol, timeframe, limit)
        if df is None or len(df) < 500:
            print(f"âŒ DonnÃ©es insuffisantes")
            return None
        print(f"âœ… {len(df)} bougies tÃ©lÃ©chargÃ©es")

        # 2. CALCULATE INDICATORS
        print(f"\n[2/5] ğŸ§® Calcul indicateurs techniques...")
        df = self._calculate_all_indicators(df)
        if df is None:
            print(f"âŒ Erreur calcul indicateurs")
            return None
        print(f"âœ… Indicateurs calculÃ©s")

        # 3. EXTRACT FEATURES
        print(f"\n[3/5] ğŸ”¬ Extraction features ML...")
        features_list, valid_indices = self._extract_features_batch(df)
        if len(features_list) == 0:
            print(f"âŒ Aucune feature extraite")
            return None
        print(f"âœ… {len(features_list)} features extraites")

        # 4. LABELIZE
        print(f"\n[4/5] ğŸ·ï¸  Labellisation TP/SL...")
        labels_list = self._labelize_batch(df, valid_indices)
        print(f"âœ… {len([l for l in labels_list if l != -1])} samples labellisÃ©s")

        # Filtre les samples non labellisÃ©s
        valid_samples = [(f, l) for f, l in zip(features_list, labels_list) if l != -1]
        if len(valid_samples) < ml_config.DATASET_MIN_SIZE:
            print(f"âŒ Dataset trop petit ({len(valid_samples)} < {ml_config.DATASET_MIN_SIZE})")
            return None

        features_array = np.array([s[0] for s in valid_samples])
        labels_array = np.array([s[1] for s in valid_samples])

        # 5. SAVE
        print(f"\n[5/5] ğŸ’¾ Sauvegarde dataset...")
        filename = self._save_dataset(features_array, labels_array, symbol, timeframe)
        if filename:
            print(f"âœ… Dataset sauvegardÃ©: {filename}")
            self._print_dataset_stats(features_array, labels_array)
            return filename
        else:
            print(f"âŒ Erreur sauvegarde")
            return None

    def build_multi_dataset(self, symbols: List[str], timeframe: str, limit_per_symbol: int = 20000) -> Optional[str]:
        """
        Construit un dataset MULTI-CRYPTO (combine plusieurs symboles)

        Args:
            symbols: Liste de symboles (ex: ['ETH/USDT', 'BTC/USDT', 'BNB/USDT'])
            timeframe: Timeframe
            limit_per_symbol: Bougies par symbole

        Returns:
            Chemin vers le fichier dataset crÃ©Ã©
        """
        print(f"\n{'='*70}")
        print(f"ğŸŒ CONSTRUCTION DATASET MULTI-CRYPTO".center(70))
        print(f"{'='*70}")
        print(f"\nğŸª™  Symboles: {', '.join(symbols)}")
        print(f"ğŸ“Š {limit_per_symbol} bougies par symbole")

        all_features = []
        all_labels = []

        for i, symbol in enumerate(symbols, 1):
            print(f"\n{'â”€'*70}")
            print(f"[{i}/{len(symbols)}] Processing {symbol}...")
            print(f"{'â”€'*70}")

            # Build dataset pour ce symbole
            temp_filename = self.build_dataset(symbol, timeframe, limit_per_symbol)
            if temp_filename:
                # Charge le dataset temporaire
                with open(temp_filename, 'rb') as f:
                    data = pickle.load(f)
                    all_features.append(data['features'])
                    all_labels.append(data['labels'])

                # Supprime le fichier temporaire
                os.remove(temp_filename)
                print(f"âœ… {symbol}: {len(data['labels'])} samples ajoutÃ©s")
            else:
                print(f"âš ï¸  {symbol}: Ã©chec, ignorÃ©")

            # Pause pour Ã©viter rate limit
            if i < len(symbols):
                time.sleep(2)

        if len(all_features) == 0:
            print(f"\nâŒ Aucun dataset gÃ©nÃ©rÃ©")
            return None

        # Combine tous les datasets
        print(f"\n{'='*70}")
        print(f"ğŸ”— FUSION DES DATASETS".center(70))
        print(f"{'='*70}")

        combined_features = np.vstack(all_features)
        combined_labels = np.concatenate(all_labels)

        # Shuffle pour mÃ©langer les cryptos
        indices = np.random.permutation(len(combined_labels))
        combined_features = combined_features[indices]
        combined_labels = combined_labels[indices]

        # Sauvegarde
        filename = self._save_dataset(
            combined_features,
            combined_labels,
            f"MULTI_{len(symbols)}cryptos",
            timeframe
        )

        if filename:
            print(f"\nâœ… Dataset multi-crypto sauvegardÃ©: {filename}")
            self._print_dataset_stats(combined_features, combined_labels)
            return filename
        return None

    def _fetch_historical_data(self, symbol: str, timeframe: str, limit: int) -> Optional[pd.DataFrame]:
        """Fetch donnÃ©es historiques via ccxt"""
        try:
            # ccxt limite Ã  1000 bougies par appel, on doit faire plusieurs appels
            all_data = []
            remaining = limit
            last_timestamp = None

            while remaining > 0:
                fetch_limit = min(remaining, 1000)

                # Fetch avec timestamp si on continue
                if last_timestamp:
                    ohlcv = self.data_collector.exchange.fetch_ohlcv(
                        symbol, timeframe, since=last_timestamp, limit=fetch_limit
                    )
                else:
                    ohlcv = self.data_collector.exchange.fetch_ohlcv(
                        symbol, timeframe, limit=fetch_limit
                    )

                if not ohlcv:
                    break

                df_chunk = pd.DataFrame(
                    ohlcv,
                    columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']
                )
                df_chunk['timestamp'] = pd.to_datetime(df_chunk['timestamp'], unit='ms')

                all_data.append(df_chunk)
                remaining -= len(df_chunk)
                last_timestamp = int(df_chunk.iloc[-1]['timestamp'].timestamp() * 1000) + 1

                print(f"   TÃ©lÃ©chargÃ©: {len(df_chunk)} bougies (total: {len(pd.concat(all_data))})")

                # Pause pour rate limit
                time.sleep(0.5)

                # Si on a reÃ§u moins que demandÃ©, on a atteint la fin
                if len(df_chunk) < fetch_limit:
                    break

            if all_data:
                df_final = pd.concat(all_data, ignore_index=True)
                # Remove duplicates
                df_final = df_final.drop_duplicates(subset=['timestamp'])
                df_final = df_final.sort_values('timestamp').reset_index(drop=True)
                return df_final
            return None

        except Exception as e:
            self.logger.error(f"Erreur fetch data: {e}")
            print(f"âŒ Erreur: {e}")
            return None

    def _calculate_all_indicators(self, df: pd.DataFrame) -> Optional[pd.DataFrame]:
        """Calcule tous les indicateurs nÃ©cessaires"""
        try:
            df = self.indicators.add_all_indicators(df)
            return df
        except Exception as e:
            self.logger.error(f"Erreur calcul indicateurs: {e}")
            return None

    def _extract_features_batch(self, df: pd.DataFrame) -> Tuple[List, List]:
        """Extrait les features pour toutes les bougies valides"""
        features_list = []
        valid_indices = []

        # Commence Ã  partir de l'index 200 pour avoir assez d'historique
        for idx in range(200, len(df)):
            # Extrait un subset du DataFrame jusqu'Ã  idx
            df_subset = df.iloc[:idx+1].copy()

            # S/R detection
            sr = self.sr_detector.detect(df_subset)
            sr_dict = {
                'support': sr['nearest_support'],
                'resistance': sr['nearest_resistance']
            }

            # Extract features
            features = self.feature_extractor.extract_features(df_subset, sr_dict)
            if features is not None:
                features_list.append(features)
                valid_indices.append(idx)

            # Progress
            if (idx - 200) % 1000 == 0:
                print(f"   Features extraites: {len(features_list)}/{len(df)-200}")

        return features_list, valid_indices

    def _labelize_batch(self, df: pd.DataFrame, indices: List[int]) -> List[int]:
        """Labellise tous les samples"""
        labels = []
        for idx in indices:
            label = self._label_sample(df, idx)
            labels.append(label)

            if len(labels) % 1000 == 0:
                wins = len([l for l in labels if l == 1])
                losses = len([l for l in labels if l == 0])
                print(f"   LabellisÃ©s: {len(labels)} (WIN: {wins}, LOSS: {losses}, SKIP: {len(labels)-wins-losses})")

        return labels

    def _label_sample(self, df: pd.DataFrame, idx: int) -> int:
        """
        Labellise un sample (WIN/LOSS)

        Logique:
        - Regarde les N prochaines bougies
        - Si TP atteint avant SL â†’ WIN (label 1)
        - Si SL atteint avant TP â†’ LOSS (label 0)
        - Si ni l'un ni l'autre â†’ pas de label (-1, skip)

        Args:
            df: DataFrame complet
            idx: Index de la bougie Ã  labelliser

        Returns:
            1 (WIN) ou 0 (LOSS) ou -1 (pas de label)
        """
        entry_price = df.iloc[idx]['close']
        tp_price = entry_price * (1 + ml_config.LABEL_TP_PERCENT)
        sl_price = entry_price * (1 - ml_config.LABEL_SL_PERCENT)

        max_candles = ml_config.LABEL_MAX_CANDLES
        end_idx = min(idx + max_candles, len(df))

        for i in range(idx + 1, end_idx):
            high = df.iloc[i]['high']
            low = df.iloc[i]['low']

            # Check si TP atteint
            if high >= tp_price:
                return ml_config.LABEL_WIN  # 1

            # Check si SL atteint
            if low <= sl_price:
                return ml_config.LABEL_LOSS  # 0

        # Ni TP ni SL atteint dans le dÃ©lai
        return -1  # Skip ce sample

    def _save_dataset(self, features: np.ndarray, labels: np.ndarray, symbol: str, timeframe: str) -> Optional[str]:
        """Sauvegarde le dataset"""
        try:
            # Nom de fichier
            symbol_clean = symbol.replace('/', '_')
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{ml_config.DATASET_OUTPUT_DIR}dataset_{symbol_clean}_{timeframe}_{timestamp}.pkl"

            # Package
            dataset = {
                'features': features,
                'labels': labels,
                'feature_names': ml_config.FEATURE_NAMES,
                'symbol': symbol,
                'timeframe': timeframe,
                'created_at': datetime.now().isoformat(),
                'config': {
                    'tp_percent': ml_config.LABEL_TP_PERCENT,
                    'sl_percent': ml_config.LABEL_SL_PERCENT,
                    'max_candles': ml_config.LABEL_MAX_CANDLES
                }
            }

            # Save
            with open(filename, 'wb') as f:
                pickle.dump(dataset, f)

            self.logger.info(f"Dataset sauvegardÃ©: {filename}")
            return filename

        except Exception as e:
            self.logger.error(f"Erreur sauvegarde dataset: {e}")
            return None

    def _print_dataset_stats(self, features: np.ndarray, labels: np.ndarray):
        """Affiche les statistiques du dataset"""
        wins = np.sum(labels == 1)
        losses = np.sum(labels == 0)
        total = len(labels)
        win_rate = (wins / total * 100) if total > 0 else 0

        print(f"\n{'='*70}")
        print(f"ğŸ“Š STATISTIQUES DATASET".center(70))
        print(f"{'='*70}")
        print(f"\nğŸ“ˆ Samples totaux: {total:,}")
        print(f"âœ… WIN (TP atteint): {wins:,} ({win_rate:.1f}%)")
        print(f"âŒ LOSS (SL atteint): {losses:,} ({100-win_rate:.1f}%)")
        print(f"ğŸ§¬ Features: {features.shape[1]} dimensions")
        print(f"\nğŸ’¡ Balance: {'âœ… Ã‰quilibrÃ©' if 40 < win_rate < 60 else 'âš ï¸ DÃ©sÃ©quilibrÃ©'}")
        print(f"{'='*70}")


# Test/Run
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='APEX ML Dataset Builder')
    parser.add_argument('--symbol', type=str, default='ETH/USDT', help='Symbole (ex: ETH/USDT)')
    parser.add_argument('--multi', type=str, help='Plusieurs symboles sÃ©parÃ©s par virgule (ex: BTC/USDT,ETH/USDT,BNB/USDT)')
    parser.add_argument('--timeframe', type=str, default='1m', help='Timeframe (ex: 1m, 5m)')
    parser.add_argument('--limit', type=int, default=50000, help='Nombre de bougies')

    args = parser.parse_args()

    print("ğŸ”¨ APEX ML Dataset Builder")
    print("="*70)

    builder = DatasetBuilder()

    if args.multi:
        # Mode multi-crypto
        symbols = [s.strip() for s in args.multi.split(',')]
        dataset_file = builder.build_multi_dataset(symbols, args.timeframe, args.limit)
    else:
        # Mode single crypto
        dataset_file = builder.build_dataset(args.symbol, args.timeframe, args.limit)

    if dataset_file:
        print(f"\nğŸ‰ SUCCESS! Dataset prÃªt pour entraÃ®nement")
        print(f"ğŸ“ Fichier: {dataset_file}")
        print(f"\nğŸ’¡ Prochaine Ã©tape:")
        print(f"   python train_ml_model.py --dataset {dataset_file}")
    else:
        print(f"\nâŒ Ã‰chec crÃ©ation dataset")
